---
title: "MongoDB 4.0 의 Replication"
categories: 
- NoSQL
excerpt: |
  Replication 은 동일한  데이터 세트를 유지하는 그룹을 말한다.
  몽고 디비에서 replication 은 중복된 데이터로 아래와 같은 고 가용성을 제공한다. 
  내결함성 (Fault Tolerance) 
  백업의 용도로 유지관리
  클라이언트의 많은 읽기 요청을 분산하여 읽기 성능 향상
feature_text: |
  MongoDB 4.0 의 Replication 에 대해서
feature_image: "https://picsum.photos/2560/600?random"
---

* table of contents
{:toc .toc}

**Replication** 은 동일한  `데이터 세트`를 유지하는 그룹을 말한다.
몽고 디비에서 replication 은 중복된 데이터로 아래와 같은 고 가용성을 제공한다. 
* **내결함성** (Fault Tolerance) 
* **백업의 용도**로 유지관리
* 클라이언트의 많은 읽기 요청을 분산하여 읽기 성능 향상


## Replica set 구성
하나의 Primary 노드와 여러개의 Secondary 노드로 이루어져있다. 
![Alt text](https://monosnap.com/image/Wk6FhSQZTGmgTausMHOL1moCJMEjxf))

* **Primary 노드**
	* 무조건 replica set 에 1개의 Primary 존재
	* 모든 Write 작업을 받는다.
	* 데이터의 모든 변경사항을 기록한다 (oplog)
* **Secondary 노드**
	* oplog 비동기 복제하여 데이터 셋에 적용
	* primary 가 사용불가능할 때 primary를 뽑기 위한 선거
	* 특별히 추가 설정가능. 
		* 선거에서 Primary 가 무조건 되지 않도록 (우선순위 0 순위)
		* Hidden 기능으로 일반 트래픽에서 분리
		* 실수로 삭제하는것과 같은 특정 오류에서 복구시킬 수 있도록 history 스냅샷 남김
		* 투표권이 없도록
* **중재자 (Arbiter)**
	* 데이터 복제 하지 않음
	* Heartbeat 과 선거 참여
	* 주로 **짝수**의 Secondary 노드가 있을때 추가됨
	* 데이터를 다루지 않으므로 cpu 리소스가 적어도 가능
	* 전용 하드웨어가 없어도 가능
	* 데이터를 다루지 않으므로 중재자 추가로 인해 replica set 에 오버헤드 없음
	* 중재자는 primary, secondary 가 호스팅되고 있는 서버에서 실행되면 안됨

**최소 권장 구성**
: **3개**의 노드 (Primary 1, Secondary 2)

**최대 구성**
: 노드개수 **50개** 이지만 여기서 **최대 7개**의 노드만 선거권을 가짐

## 자동 Failover
Primary 서버가 `electionTimeoutMillis` (default: 10초) 동안 통신을 하지 않으면 선거요청이 시작되고 새로운 서버를 Primary 서버로 지정하게된다. 
* Primary 가 없으므로 선거가 끝날때 까지 Write 작업이 이루어지지 않는다.
* 그러나 Secondary 서버는 있으므로 Read 작업은 계속 이루어진다.

클러스터가 새로운 Primary 를 뽑는데 걸리는 시간[**Primary 를 사용할수 없음 표시 ~ 선거 호출 ~ 완료**]은 일반적으로 평균 12 초를 넘기면 안된다. 이 값은 역시 기본값일 뿐 `electionTimeoutMillis` config 가능

*  `electionTimeoutMillis`가 10초 보다 길어질 경우
	*   클러스터에서 Primary 기능을 사용하지 못하는 시간도 길어짐

* `electionTimeoutMillis`가 10초 보다 낮을 경우
	* 실패를 더 빠르게 감지할수는 있으나
	* Primary 네트워크가 정상이어도 Secondary 가 네트워크 대기시간으로 10초를 넘기게되면
	* 선거과정과 Primary 선출이 자주 일어나게 되는 단점


> **MongoDB 3.6 + 버전**
> Primary 가 사용불가능한 것을 감지하고 자동으로 특정 write  operations 는 retry 한다.
> 자동으로 failovers 와 선거과정이 이루어진다.     

## 그 이외의 노드

> **Priority 0 이란?**
>  선거를 시작시킬수도, 선거에서 Primary 가 될 수도 없는 노드.
>  그러나 일반 Secondary 노드 처럼 선거에서 투표권은 존재, 데이터 복제, read 동작 수행.
>  **목적 1**: 특별히 `multi-data center 데이터 분산`에 유용한 Secondary 노드인 경우 Primary 가 되지 않도록.
>  ![Alt text](https://monosnap.com/image/a0EpG3DJ9jO5Yp71tf8ddFZAjGN6Ac)
>  **목적 2**: Standby 용도의 Secondary 노드인경우 빠르게 새 구성원을 대체하기위해 현재 복사본만 유지하면서 Standby 로 존재. 일반적으로 Standby 라도 Priority 를 0 으로 줄 필요는 없지만 위와 같은 지리적 분산 또는 하드웨어 분산을 위해서 설정 (이 경우 Priority 0 방법 대신 Hiiden 기능으로 분리도 가능)
>  
>  **Secondary 가 priority 0 을 갖도록 (투표권만 갖는 상태) 설정할 경우 Failover 에 대비**해서 고려할 것들
>  * 메인 데이터 센터가 투표권을 가지는 노드들과 Primary 가 될 수 있는 노드들이 반드시 존재하는지 확인
>  * 네트워크에서 일어날 수 있는 모든 failover 를 고려해야함
>    
>    ---
>  **Hidden 이란?**
>  Primary 의 **데이터 셋**은 복사하고 있지만 클라이언트에게 **`보이지 않는 상태`**. 
>  replica set 에서 다른 노드들과 다른 패턴을 가진 **workload** 에 적합하다. Hidden 노드는 항상 priority 0 이다. 
>  투표권이 있을 수 있다. 
>  ![Alt text](https://monosnap.com/image/EeXYYcyCHTeL70Z6qkB8WmI1slN5sn)
>  클라이언트 쪽에서 알 수 없으니 읽기 동작도 수행하지 않는다. 즉 복제 이외의 트래픽은 없다. 
>  **목적**: Reporting, Backup 과 같은 전용 테스크를 수행하는 용도로 사용
>  ---
>  **Delayed 란?**
>  replica set의 데이터 복제본을 가지고 있지만 **지연**이 있어서 최근의 변화가 적용되지 않은 과거 상태의 데이터를 가지고 있는 노드를 말한다. 
>  delayed 노드는 **과거의 스냅샷 역할**을 하고 **rolling backup** 이라고 볼 수 있기 때문에 human 에러 발생 시 복구를 도와줄 수있는 노드가 된다.
>  그러나 이전의 데이터를 들고 있기 때문에 **아래와 같은 상태**로 만들어줘야한다.
>  * `priority 0` 이어야하고 (거짓 데이터를 가진 노드가 Primary 가 되면 안되기 때문)
>  * `hidden` 노드여야 하고 (거짓 데이터를 클라이언트가 read 하면 안되기 때문)
>   

## 비동기 Oplog 복제
Oplog(Operation Log) 는 데이터를 수정하는 모든 작업을 **롤링 기록한 capped collection** 이다. (capped collection 이지만 oplog 만 mongoDB 4.0 부터는 제한된 사이즈를 초과하는게 가능해졌다.)

동작 순서는 아래와 같다.
1. mongoDB 가 데이터베이스 동작을 Primary 에 적용함
2. 그 후 primary 의 oplog에 기록
3. Sencondary 노드가 비동기적으로 oplog 를 복제 후 operation 을 적용
 
복제를 용이하게 하기 위해 replica set 구성 노드들은 heartbeat 를 서로 주고받다가 oplog 를 **다른 노드**로 부터 받기도 한다. oplog 안에 있는 각각의 동작은 멱등하기 때문에 oplog 동작은 한번 적용되든 여러번 적용되든 같은 결과가 나온다.

> **Oplog 사이즈**
> oplog 사이즈를 지정하지 않고 처음 replica set 을 시작하면 MongoDB 는 default 사이즈로 oplog 를 생성한다. 
> **윈도우** 또는 **Unix** 계열 OS의 경우
> ![Alt text](https://monosnap.com/image/XxySAIveRdS6NlCpVqlsg2DbjXq9mb)
> 64 bit 의 **MacOS** 의 경우
> ![Alt text](https://monosnap.com/image/mYMQJHcVlL8rUPOXLxH0UkVGBxE7rM)
> 대부분의 경우 기본으로 주어지는 oplog 사이즈가 **충분**하지만 아래와 같은 workloads 라면 **사이즈 업**을 고려해야 한다.
> 1. 한번에 여러개의 documents 를 업데이트 할 때
>  * oplog 는 멱등해야하기 때문에 multiple 업데이트를 각각 개별적인 업데이트로 변경해 저장한다.
> 2. Insert 된 양과 같은 양의 데이터를 지우는 경우
> * 1번과 같은 이유와 추가적으로 이 경우는 더 많은 데이터일 가능성이 있다.  
> 3. Update 가 굉장히 많은 부분을 차지하는 경우
> 
> 반대로 만약에 write 작업은 적고 read 작업만 주로 이뤄진다면 더 작은 oplog 사이즈를 가져도 무방하다

### 데이터 동기화
MongoDB 는 두종류의 데이터 복제를 사용한다
1. 초기 동기화를 통한 새 노드에 데이터 셋 채우기
2. 복제를 통한 전체 데이터 셋 지속적인 변경 적용

#### 초기 동기화(Initial Sync)
initial sync 를 통해 replica set 중 하나의 노드에서 모든 데이터를 복사한다.
1. 로컬 데이터 베이스를 제외한 모든 데이터베이스를 clone 해서 
2. 데이터셋에 source 로부터 가져온 oplog 를 사용해서 모든 변화를 적용한다. 현재 상태로 만듦.

작업 도중 fail 이 나면 initial sync 자체의 retry 로직을 이용해 복구한다.
#### 복제 (Replication)
Secondary 노드는 데이터를 초기 동기화 이후에 계속 복제한다. Secondary 노드는 source 에서 oplog 를 가져와서 현재 상태로 적용한다. 이 작업은 자동으로 이루어 지고 **아래를 기준**으로 일어난다.
* 핑(heartbeat) 시간의 변화
* 다른 노드(not delayed and hidden)의 복제 상태


## 고 가용성 설정
### replica set 선거
어떤 노드가 Primary 노드가 될 것인지 투표를 하게 된다. 이 선거는 아래와 같은 상황에 트리거링 된다.
* replica set 에 새로운 노드를 추가할 때
* replica set 을 초기화 시킬 때
* rs.stepDown() or rs.reconfig()
* secondary 노드가 primary 노드와 timeout 시간(기본 10초) 동안 연결이 안될 경우
![Alt text](https://monosnap.com/image/XsVBwFCb7jUl5V2dy3y7X0PO1VmJPa)

> **HeartBeat**
> Replica set 의 노드들은 2초마다 heartbeat(ping) 를 보내는데 timeout 시간동안 리턴되지 않으면 연결 불가라고 판단.
> 
> ---
> **Primary 선거 알고리즘**
> 사용가능한 가장 Priority 가 높은 노드를 투표한다. 


### Replica set failover 동안 rollback 하기

rollback 은 failover 가 일어난 뒤 
<U> replica set 에 노드가 다시 들어왔을때 **이전 Primary 에게 일어난 write 동작을 되돌리는**것</U>. 
이 롤백과정이 필요한 상황은 
* Primary 는 write 동작을 받아들인 상태에서 
* secondary 들이 아직 복사를 못했는데 
* Primary 가 내려간 경우이다. 

Primary 가 Secondary 로 다시 들어오게 되면 **다른 노드들과 일관성**을 맞추기 위해 그 write 동작이 롤백된다. 

MongoDB 는 Rollback 을 피하려고 한다. 
롤백이 일어나는 때는 보통 **네트워크** 때문에 일어난다.

롤백이 일어나면 몽고디비는 default 로 롤백 데이터를 dbpath 의 /rollback 폴더에 BSON 으로 남긴다. 
``` plain
<database>.<collection>.<timestamp>.bson
records.accounts.2011-05-09T18-10-04.0.bson
```


replica set 은 default 로 **write concern** 이 `{w:1}` 로 되어있어서 **write 동작에대한 Primary의  ack** 만 제공된다. 이런 기본 write concern 은 Primary 의 write 동작을 secondary 가 복제하지 못한 채로 내려가면 데이터 롤백이 일어나게 되는데 multi document 트랜잭션에 쓰여지는 데이터도 포함된다. 

클라이언트한테 ack 가 간 데이터의 롤백을 막으려면 투표권이 있는 노드들을 Journal enable 을 켜둬야 한다. 그리고 클라이언트한테 ack 를 보냄으로써 write 동작이 replica set 의 대부분 노드에게 전파되었음을 보장하기 위해 `w: majority` write concern (뒤에 write concern 부분 설명 참고) 을 사용해야 한다. 

> writeConcernMajorityJournalDefault 가 true 가 될 경우
> : write 동작이 과반수 노드의 **disk 상의 Journal** 에 기록되는것을 기다린 다음 ACK
> 
> false 가 될 경우
> : write 동작이 과반수 노드의 메모리에 적용된 것을 기다린 다음 ACK 
>  disk 에 쓰여지는 것을 기다리지 않기 때문에 롤백될 가능성이 있다.
> 
> ---
> **Journaling**
> 디스크에 쓰여진 Journal 을 보고 자동 복구하는 작업

**롤백을 피해야 하는 이유**
write concern 이 뭐건간에 local 또는 available **read concern** 을 사용하는 클라이언트의 경우 write 동작이 ack 가 오기 전에도 write 가 된 결과를 미리 볼 수 있게 된다. multi document 트랜젝션 동작인 경우에도 데이터는 그 트랜젝션이 커밋 되기전에 반영되거나 보이지 않지만 이런 설정을 가지고 있는 클라이언트는 미리 그 결과가 반영된 데이터를 보게된다. 그 이후에 롤백이 되면 결국 잘못되고 일관성이 없는 데이터를 보여주게 된다.

> mongoDB 4.0 + 부터는 롤백 데이터에 제한이 없지만
> 이전버전은 300MB 이상의 데이터를 롤백할 수 없고 이 경우 수동으로 작업해줘야 한다.


## Replica set 의 read 와 write
클라이언트 어플리케이션의 관점에서 mongoDB 인스턴스가 단일 서버에서 동작중인지 또는 replica set 에서 동작하는 지는 투명하다. 그러나 mongoDB 는 replica set 의 read write 설정을 추가로 해줄 수 있다.

### write concern
write concern 은 write 동작에대해 **몽고디비가 원하는 ack 레벨**을 뜻한다.
기본 write concern 은 primary 만 ack 를 받는 것이다. 
![Alt text](https://monosnap.com/image/A8j6eqxCOcxXi4xeT6GgaEmgxeBv5R)

이 기본 write concern 변경은 write 동작마다 오버라이드를 해줄 수 있다.
```
db.products.insert(
   { item: "envelopes", qty : 100, type: "Clasp" },
   { writeConcern: { w: 2, wtimeout: 5000 } }
)
```

**기본 설정 바꾸기**
`settings.getLastErrorDefaults` 를 설정함 으로써 기본 wirte concern 을 바꿀 수 있다.
```
cfg = rs.conf()
cfg.settings.getLastErrorDefaults = { w: "majority", wtimeout: 5000 }
rs.reconfig(cfg)
```

### read preference
read preference 는 클라이언트의 **read 동작을 replica set 의 다른 노드로 라우트** 하는 설정을 말한다.
![Alt text](https://monosnap.com/image/hwJzN0D0kVRYySESsZWstJwyIbIMhl)

| Read Preference Mode | 설명|
|---|---|
| Primary | `Default` <br> Primary 로 부터 read 가 이뤄진다. <br> 읽기 동작을 포함하는 Multi-document 트랜젝션은 Primary 모드를 사용해야 한다. <br> (하나의 트랜젝션 안에있는 모든 operation 은 무조건 같은 노드로 라우트 되어야 하기 때문)|
| **PrimaryPreferred** | 대부분 Primary 에서 읽지만 사용할 수 없을 경우 Secondary 노드에서 읽게된다.|
| Secondary | 모든 작업이 Secondary 노드에서 읽힌다. |
| Secondary Preferred | 대부분 Secondary 에서 읽히지만 Secondary 가 없을 때 Primary 에서 읽는다 |
| nearest | 노드의 유형에 상관 없이 네트워크 대기시간이 가장 적은 노드에서 읽는다. |


읽기 preference 를 Primary 가 아닌 것(secondary, secondary preferred, nearest)으로 설정하면 다음과 같은 **`장점`**이 있다.
* 대기 시간이 짧은 가까운(가깝다는 의미는 데이터 센터가 여러개 있을때 현재 데이터센터를 말한다) 노드에서 읽을 수 있다.
* 장애발생시 선거가 진행 되는 등 Primary 가 **이용불가능한 상태일때 Read 가 가능**하다.

그러나 다음과 같은 **`단점`**이 있다.
* replica set의 모든 노드는 결과적으로 계속 복제하고 있기 때문에 쓰기 트래픽은 비슷하다. 빨리 복제하기 때문에 거의 동일한 속도로 읽기를 처리하긴 하지만, 그 **지연시간 동안 다른 데이터를 반환**할 수도 있다.
* replica set 의 다른 노드가 모든 어플리케이션의 요청을 처리할 수 있어야 하기 때문에 read 동작을 secondary 에 분산하면 가용성이 떨어질 수 있다. (모든 노드가 바빠지므로)

그래서 **정확한 데이터 반환용, Read 트래픽 분산용도로 read preference 를 변경하면 안된**다.
만약 트래픽 수용량을 늘리기 위함이라면 **sharding** 을 해야한다.


> **MaxStalenessSeconds**  (MongoDB 3.4 +)
> Replica set 노드는 네트워크 정체, 낮은 디스크 처리량, 장기 실행 작업 등으로 인해 Primary 보다 뒤쳐질 수 있다. Read preference 의 `maxStalenessSeconds` 옵션은 Secondary 로 부터 읽기 작업을 위해 최대 복제 지연(or staleness-탁함)을 지정하는 옵션이다. Secondary  의 Staleness 가 `maxStalenessSeconds` 를 초과하면 클라이언트는 읽기작업을 해당 Secondary 노드에서 하지 않는다.
> 이 옵션은 Secondary 서버에서 읽기위한 옵션이므로 Primary read preference 를 제외한 모든 모드에서 가능하다.
> default 는 제한하지 않는다.
> 최신 쓰기 시간을 주기적으로 확인은 하지만 빈도가 낮기 때문에 정확한 값으로 하긴 어렵다. 
> 이 값은 90초 또는 그 이상이어야 한다. (그 이하로 하면 오류)
> 


## Read 작업
기본적으로는 Primary 에서 Read 가 이뤄지는데 아래와 같이 설정을 통해 변경할 수 있다.
![Alt text](https://monosnap.com/image/B2S0YeonfChePOZ03KPsQLlxFtyF2Z)

그러나 복제는 비동기로 진행되기때문에 만약 Secondary 에 없는 데이터를 요구하게되면 실제로 Primary 에 있음에도 없다고 리턴하게 될 수 있다.

Read concern 설정에 따라 Write 가 durable(셧다운 또는 충돌로 재실행 대기) 상태 중에도 Write 의 결과를 볼 수 있는 설정도 있다. 
![Alt text](https://monosnap.com/image/amJD9HgSw1vYpvxWWBuuvxjF2CWEDr))

위와 같은 설정처럼 **write concern 설정에 관계 없이** "local", "available" readConcern 을 사용하는 클라이언트라면 write 동작에 대한 ack 가 오기도 전에 write 의 결과를 볼 수 있다.

## Repilca set 의 아키텍쳐 전략
Repilca set의 아키텍쳐는 데이터의 용량과 능력에 영향을 미친다. 

상용 시스템의 replica set 배포의 표준은 3 노드로 이루어진 replica set 이다. 이 구조는 데이터 중복성과 내결함성을 지원한다. 

**맴버수를 결정해라**
투표권이 있는 최대 수는 7을 넘길 수 없다

**홀수의 노드를 배포해라**
replica set 이 홀수의 노드를 가지고 있는지 확인하라 만약 짝수를 가진다면 중재자를 하나 추가해서 홀수의 투표권을 만들어라
중재자는 어플리케이션 서버에 같이 띄워도 무방하다. 그러나 일반적으로 replica set 당 하나 이상의 중재자는 없어야 한다.

**내결함성을 고려하라**
replica set 에서 내결함성이란 곧 노드가 unavailable 상태가 되어도 여전히 Primary 로 뽑힐 수 있는 여분의 노드라고 할 수 있다. 다시말하면 replica set 안의 노드 수와 투표권이 있는 노드의 과반수 사이의 차이다. 

| Replica set의 노드 수 | 새 Primary 가 뽑힐 수 있는 과반수 | 내결함성 |
|---|---|---|
|3|2|1|
|4|3|1|
|5|3|2|
|6|4|2|

* 노드를 하나 추가한다고 **항상** 내결함성이 증가하는 건 아니다.

**Dedicated 한 일을 위해 Hidden, Delayed 노드를 사용하라**
backup or reporting

**Read 작업이 헤비한 경우 Load Balance 해라**
read 작업을 Secondary 에 분산함으로써 read 처리량을 개선할 수 있다. 

**하나의 데이터 센터보다 두개의 데이터 센터에 배포해라**
두개의 데이터 센터에 배포하면 이점이 있다. 
* 만약 데이터 센터 하나가 내려가도 데이터는 여전히 사용가능하다.
* 노드 수가 적은 데이터 센터가  내려가도 replica set 은 write, read 가 가능하다.
* 그러나 만약 많은 노드를 가지고 있는 데이터센터가 내려가게 되면 replica set 은 read 만 가능해질 수 있다.

가능하다면 최소 세개의 데이터센터에 노드를 분산시키면 좋다. config server replica set 을 위해 가장 좋은 방법은 세개의 센터에 걸쳐 배포하는 것이다. 비용이 터무니 없다면 그나마 좋은 방법은 균등하게 데이터 베어링 노드를 두개의 데이터 센터에 균등하게 분배하고 나머지 노드를 클라우드에 배포하는 방법이다. (이 방법은 회사 정책이 허락한다면 하라)

항상 Primary 를 뽑을 수 있는지 확인하라.

**수요가 넘치기 전에 미리 용량을 추가하라**
수요가 넘치기 전에 새 노드를 추가해야 하는데 replica set 의 존재하는 노드는 새로운 노드가 추가되면 지원할 여유의 용량을 가져야 한다.

**지리적으로 노드를 배포하라**
데이터센터가 내려갈 경우에 대비해 지리적으로 다른곳에 배포하는것이 좋다. 

**DNS 호스트 네임을 써라**
replica set 을 구성할 때 가능하면 ip 주소보다 논리적 DNS 호스트네임을 쓰는것이 좋다. ip 주소는 운영상 변경될 수 도 있지만 DNS 는 변경될 상황이 보통 없기 때문이다.
 
